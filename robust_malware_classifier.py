#!/usr/bin/env python
"""
Run example:
    python robust_malware_classifier.py \
        --epochs 120 \
        --pca_components 120 \
        --pgd_steps 7 \
        --adv_sample_weight 0.2
"""

import argparse
import pickle
import random
from pathlib import Path
from typing import Tuple

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler

# -------------------------------------------------
# Reproducibility
# -------------------------------------------------
RNG_SEED = 42
torch.manual_seed(RNG_SEED)
np.random.seed(RNG_SEED)
random.seed(RNG_SEED)

# -------------------------------------------------
# Data utilities
# -------------------------------------------------

def load_datasets(data_dir: Path) -> Tuple[pd.DataFrame, list, np.ndarray]:
    """Load clean & adversarial datasets and return merged df, feature columns, & adv‑mask."""
    clean_path = data_dir / "merged_vectors_filtered.csv"
    adv_path = data_dir / "darkhotel_adv_vectors.csv"

    if not clean_path.exists() or not adv_path.exists():
        raise FileNotFoundError(f"CSV files not found in {data_dir}")

    df_clean = pd.read_csv(clean_path)
    df_adv = pd.read_csv(adv_path)

    feature_cols = [c for c in df_clean.columns if c not in ("filename", "label")]
    if len(feature_cols) != 500:
        raise ValueError(f"Expected 500 features but got {len(feature_cols)}")

    # Bring adv file into the same layout as the clean file
    df_adv_feats = df_adv.drop(columns=["sample_idx"]).copy()
    df_adv_feats.columns = feature_cols  # rename to canonical names
    df_adv_feats["label"] = "DarkHotel"
    df_adv_feats["filename"] = "adv_sample"

    df_all = pd.concat([df_clean, df_adv_feats], ignore_index=True)
    df_all[feature_cols] = df_all[feature_cols].fillna(0)

    adv_mask = df_all["filename"] == "adv_sample"
    return df_all, feature_cols, adv_mask.values


class MalwareDataset(Dataset):
    """Tiny wrapper that can optionally expose a per‑sample weight."""

    def __init__(self, X: np.ndarray, y: np.ndarray):
        self.X = torch.from_numpy(X).float()
        self.y = torch.from_numpy(y).long()

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# -------------------------------------------------
# Model definition
# -------------------------------------------------

class MLP(nn.Module):
    def __init__(self, input_dim: int, num_classes: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 1280),
            nn.BatchNorm1d(1280),
            nn.GELU(),
            nn.Dropout(0.4),
            nn.Linear(1280, 640),
            nn.BatchNorm1d(640),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(640, 320),
            nn.BatchNorm1d(320),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(320, num_classes),
        )

    def forward(self, x):
        return self.net(x)


# -------------------------------------------------
# PGD adversarial routine
# -------------------------------------------------

def pgd_attack(
    model: nn.Module,
    loss_fn: nn.Module,
    x: torch.Tensor,
    y: torch.Tensor,
    epsilon: float = 0.1,
    alpha: float = 0.02,
    steps: int = 5,
):
    x_adv = x.detach().clone()
    x_adv.requires_grad_(True)
    for _ in range(steps):
        logits = model(x_adv)
        loss = loss_fn(logits, y)
        loss.backward()
        with torch.no_grad():
            x_adv += alpha * x_adv.grad.sign()
            perturb = torch.clamp(x_adv - x, min=-epsilon, max=epsilon)
            x_adv = torch.clamp(x + perturb, min=-3.0, max=3.0)
        x_adv = x_adv.detach().requires_grad_(True)
    return x_adv.detach()


# -------------------------------------------------
# Train / eval helpers
# -------------------------------------------------

@torch.no_grad()
def evaluate(model: nn.Module, loader: DataLoader, device: torch.device):
    model.eval()
    correct = total = 0
    preds_all, labels_all = [], []
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        logits = model(x)
        preds = logits.argmax(1)
        correct += (preds == y).sum().item()
        total += y.size(0)
        preds_all.extend(preds.cpu().numpy())
        labels_all.extend(y.cpu().numpy())
    return correct / total, preds_all, labels_all


def train(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    *,
    epochs: int,
    optimizer: torch.optim.Optimizer,
    scheduler: torch.optim.lr_scheduler._LRScheduler,
    loss_fn: nn.Module,
    device: torch.device,
    epsilon: float,
    alpha: float,
    adv_frac: float,
    pgd_steps: int,
    patience: int = 12,
):
    best_val = 0.0
    no_improve = 0
    train_losses, val_accs = [], []

    scaler = torch.cuda.amp.GradScaler(enabled=device.type == "cuda")
    ema_loss = None

    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0.0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad(set_to_none=True)

            with torch.cuda.amp.autocast(enabled=device.type == "cuda"):
                logits = model(x)
                loss = loss_fn(logits, y)

            scaler.scale(loss).backward()

            # PGD adversarial training on a fraction of the batch
            if adv_frac > 0.0:
                k = int(x.size(0) * adv_frac)
                if k > 0:
                    idx = torch.randperm(x.size(0), device=device)[:k]
                    x_adv = pgd_attack(model, loss_fn, x[idx], y[idx], epsilon, alpha, pgd_steps)
                    with torch.cuda.amp.autocast(enabled=device.type == "cuda"):
                        logits_adv = model(x_adv)
                        loss_adv = loss_fn(logits_adv, y[idx])
                    scaler.scale(loss_adv).backward()

            # Gradient clipping for stability
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
            scaler.step(optimizer)
            scaler.update()

            running_loss += loss.item() * x.size(0)

        scheduler.step()
        train_loss_epoch = running_loss / len(train_loader.dataset)
        val_acc, _, _ = evaluate(model, val_loader, device)
        print(f"Epoch {epoch:03d} | train_loss {train_loss_epoch:.4f} | val_acc {val_acc:.4f}")

        train_losses.append(train_loss_epoch)
        val_accs.append(val_acc)

        if val_acc > best_val + 1e-4:
            best_val = val_acc
            no_improve = 0
            torch.save(model.state_dict(), "best_model.pt")
        else:
            no_improve += 1
            if no_improve >= patience:
                print("Early stopping (no improvement).")
                break

    with open("training_log.pkl", "wb") as f:
        pickle.dump({"loss": train_losses, "val_acc": val_accs}, f)


# -------------------------------------------------
# Main entry
# -------------------------------------------------

def main():
    p = argparse.ArgumentParser(description="Train a robust malware classifier")
    p.add_argument("--data_dir", type=str, default=".")
    p.add_argument("--epochs", type=int, default=120)
    p.add_argument("--batch_size", type=int, default=128)
    p.add_argument("--lr", type=float, default=1e-3)
    p.add_argument("--epsilon", type=float, default=0.15)
    p.add_argument("--alpha", type=float, default=0.03)
    p.add_argument("--pgd_steps", type=int, default=7)
    p.add_argument("--adv_frac", type=float, default=0.4)
    p.add_argument("--adv_sample_weight", type=float, default=0.2, help="Down‑weight factor for adversarial DH rows")
    p.add_argument("--pca_components", type=int, default=120)
    p.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    args = p.parse_args()

    # ----------------- Load data -----------------
    df_all, feature_cols, adv_mask = load_datasets(Path(args.data_dir))
    labels = sorted(df_all["label"].unique())
    label_to_idx = {lab: i for i, lab in enumerate(labels)}

    y_np = df_all["label"].map(label_to_idx).values
    X_np = df_all[feature_cols].values.astype(np.float32)

    # --------------- Train/val/test split ---------------
    X_train, X_tmp, y_train, y_tmp, adv_train, adv_tmp = train_test_split(
        X_np, y_np, adv_mask, test_size=0.2, random_state=RNG_SEED, stratify=y_np
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_tmp, y_tmp, test_size=0.5, random_state=RNG_SEED, stratify=y_tmp
    )

    # --------------- Scaling & (optional) PCA ---------------
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)

    pca = None
    if args.pca_components and args.pca_components > 0:
        pca = PCA(n_components=args.pca_components, random_state=RNG_SEED)
        X_train = pca.fit_transform(X_train)
        X_val = pca.transform(X_val)
        X_test = pca.transform(X_test)

    # Persist preprocessing artefacts
    with open("scaler.pkl", "wb") as f:
        pickle.dump(scaler, f)
    if pca is not None:
        with open("pca.pkl", "wb") as f:
            pickle.dump(pca, f)

    # --------------- Sampling strategy ---------------
    class_weights = compute_class_weight("balanced", classes=np.unique(y_train), y=y_train)
    # Base weight per sample is its class weight
    sample_weights = np.array([class_weights[label] for label in y_train], dtype=np.float32)
    # Down‑weight adversarial DH samples
    sample_weights[adv_train] *= args.adv_sample_weight
    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)

    # --------------- DataLoaders ---------------
    train_loader = DataLoader(MalwareDataset(X_train, y_train), batch_size=args.batch_size, sampler=sampler)
    val_loader = DataLoader(MalwareDataset(X_val, y_val), batch_size=args.batch_size, shuffle=False)
    test_loader = DataLoader(MalwareDataset(X_test, y_test), batch_size=args.batch_size, shuffle=False)

    # --------------- Model, optim, loss ---------------
    device = torch.device(args.device)
    model = MLP(input_dim=X_train.shape[1], num_classes=len(labels)).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=12, gamma=0.6)
    loss_fn = nn.CrossEntropyLoss()  # no class‑weights; sampling already handles imbalance

    train(
        model,
        train_loader,
        val_loader,
        epochs=args.epochs,
        optimizer=optimizer,
        scheduler=scheduler,
        loss_fn=loss_fn,
        device=device,
        epsilon=args.epsilon,
        alpha=args.alpha,
        adv_frac=args.adv_frac,
        pgd_steps=args.pgd_steps,
    )

    # --------------- Evaluation ---------------
    model.load_state_dict(torch.load("best_model.pt", map_location=device))

    test_acc, test_preds, test_labels = evaluate(model, test_loader, device)
    print(f"\n*** Final Test Accuracy: {test_acc:.4f}")
    print(classification_report(test_labels, test_preds, target_names=labels))
    print("Confusion Matrix:\n", confusion_matrix(test_labels, test_preds))

    # Accuracy on adversarial DarkHotel samples only
    adv_rows = df_all["filename"].values == "adv_sample"
    if adv_rows.sum() > 0:
        X_adv = df_all.loc[adv_rows, feature_cols].values.astype(np.float32)
        X_adv = scaler.transform(X_adv)
        if pca is not None:
            X_adv = pca.transform(X_adv)
        y_adv = y_np[adv_rows]
        adv_loader = DataLoader(MalwareDataset(X_adv, y_adv), batch_size=256, shuffle=False)
        adv_acc, _, _ = evaluate(model, adv_loader, device)
        print(f"\nAdversarial DarkHotel sample accuracy: {adv_acc:.4f}")

    test_acc, test_preds, test_labels = evaluate(model, test_loader, device)
    print(f"\n*** Final Test Accuracy: {test_acc:.4f}")
    print(classification_report(test_labels, test_preds, target_names=labels))
    print("Confusion Matrix:\n", confusion_matrix(test_labels, test_preds))

    import matplotlib.pyplot as plt
    import seaborn as sns

    cm = confusion_matrix(test_labels, test_preds)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix (Test Set)')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png')
    plt.show()

    # --------------- Save artefacts ---------------
    torch.save({"model_state": model.state_dict(), "label_to_idx": label_to_idx}, "robust_malware_model.pt")

if __name__ == "__main__":
    main()
